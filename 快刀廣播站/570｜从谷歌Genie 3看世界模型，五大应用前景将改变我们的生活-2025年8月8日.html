<!doctype html>
<html lang="zh-Hant">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>570｜从谷歌Genie 3看世界模型，五大应用前景将改变我们的生活</title>
</head>
<body>
<div class="article-cover-wrap"><img src="https://piccdn3.umiwi.com/img/202508/07/202508071035155374481516.jpeg"> <!----> <!----></div>

<div class="article-title iget-common-c1" style="-webkit-box-orient: vertical;">
    570｜从谷歌Genie 3看世界模型，五大应用前景将改变我们的生活
  </div>

<div class="article-info"><div class="author"><img src="https://piccdn3.umiwi.com/img/202405/25/202405251635229233825579.jpeg"> <span class="course-title iget-common-c3 iget-common-f4">
        快刀广播站
      </span></div> <span class="article-publish-time iget-common-c3 iget-common-f4">
      2025年8月8日 
    </span></div>

<div class="article-body"><div class="iget_rich-text-panel--container iget_rich-text-panel__large"><div class="editor-show" style="user-select: none; position: relative;"><div class="dd-audio" data-module-type="custom" style="z-index: 40; position: relative;"><div class="dd-audio-player iget-common-b7"><div class="dd-audio-info"><button class="dd-audio-icon iget-common-b10"><span class="iconfont iget-common-f4 iget-icon-play"></span></button> <div class="dd-audio-block iget-common-c2 iget-common-f5"><span class="audio-title" style="-webkit-box-orient: vertical;">从谷歌Genie 3看世界模型，五大应用前景将改变我们的生活.MP3</span> <span class="audio-duration iget-common-c3 iget-common-f6">
          11分42秒
        </span></div></div></div> <div class="audio-tips iget-common-f4 iget-common-c3">
    建议 WiFi 环境下播放
  </div></div><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">你好，我是快刀青衣。欢迎收听快刀广播站，每天带你看AI。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">今天跟大家聊一下<b>谷歌新出的世界模型Genie 3</b>。有学习圈的同学在群里点名让我讲讲这个，并且想知道世界模型到底有什么不同。你可能一听这话题，认为谷歌推出的，又是刚刚出的，自己可能用不到，就不太关心。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">其实，对于这个话题，你不必太关注这个模型版本的具体细节或它目前达到的能力。你可以带着一个问题去思考，就是<b>这种世界模型，如果再发展两年，技术能力提升100倍，成本下降到现在的1%，我们能用这个工具来干什么？</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">千万不要觉得这是痴心妄想。我们现在用的AI工具，在这两年内，技术能力可不止提升百倍，比如OpenAI的模型API的成本，大概就是前年的0.3%。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">所以，我先用简单易懂的方式跟大家描述一下这个谷歌新模型都能干啥。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这个是谷歌的DeepMind团队发布的最新基础世界模型。顺便说句题外话，我发现现在介绍大厂的产品，都不能仅仅只说大厂的名字，必须要细化到部门，这样大家才能预测这个工具在哪个领域会持续投入。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">例如，不能仅仅说字节推出的产品，而要说是飞书、扣子、剪映还是TikTok，这样就基本可以判断这个工具的服务对象是谁。类似地，不能仅仅说是腾讯的产品，而需具体到混元、腾讯云、腾讯会议或者企业微信，这样才更精确。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">我们继续回到Genie 3。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">谷歌只是发布了这个版本的一些视频，目前它仍处于完全实验室的阶段，绝大多数人都不可能用到这个工具。如果有人真的用了，那绝对是这个领域的顶级大佬。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">DeepMind团队发布的这个世界模型，结合了他们上一代的环境生成模型Genie 2和最新的视频生成模型Veo 3的最新成果。<b>这个模型可以在动态中逐帧生成画面，每秒24帧，并且在生成过程中记忆先前的状态以保持环境一致性</b>。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">前面这句话里，有好几个陌生知识点，我们展开说一下。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">视频工作者对24帧这个数字肯定很熟悉了，但是不从事这行的人，可能会纳闷为什么谷歌这个工具选择24帧作为发布标准？</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">帧，我们可以理解为视频中的一张静态图，每秒多少帧就表示一秒钟这个视频跑过去了多少张画面。24帧就是你认为自己看了一秒钟的视频，但实际上是你一秒钟看了24张跑得飞快的图片。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">当帧率低于24帧的时候，你就会感到明显的卡顿，比如早期的黑白老电影，基本是16帧的，你会感觉像在看自动播放的PPT。现在24到30帧，基本上是电影和电视短视频的标准帧率了，特别是30帧，已经是抖音、YouTube等短视频平台的常用标准。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">说完帧的概念，同学们是不是大致猜到了这个世界模型的用途？</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">其实就是用户只需提供一句文字描述或一张图片，Genie 3就可以生成一个三维环境，比如“正在爬一座喷发的火山”或“16世纪的威尼斯水城”。<b>模型会动态创建相应的场景，用户可以像玩游戏一样在里面自由移动视角。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">以前的版本Genie 2，只能生成十几秒的视频片段，而且不支持实时交互，但这次的版本，可以完成长达几分钟的实时连续交互场景。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>什么是实时连续交互？</b>就像之前的版本，是你在看别人打游戏，但在这次的版本里，你能够以第一人称视角进行体验。比如你进入到一座小镇，你向左动动鼠标，视角就移动到左边，看到左边的西部小镇的酒吧。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">前面我有提到一句“<b>生成过程中记忆先前的状态以保证环境一致性</b>”，放在小镇场景里来理解就是：因为每一帧画面都是AI实时生成的，如果我当下已经向左看到了这家酒吧，那在两分钟后，我在小路上扭头往回看，之前的位置就必须还是这家酒吧，如果要是变成了一座公共厕所，那就进入到恐怖片的领域了。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">还有一个突破就是<b>Genie 3里，并没有内置传统的物理引擎，而是在训练中让AI自己领会物体运动和碰撞的物理规律</b>。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">对DeepMind来说，这其实属于他们的舒适区。因为当年他们在做AlphaGo时，用来训练的就是人类棋谱，让AI在训练中学习人类怎么下棋的，最后战胜了李世石。但是一两年后，他们开发了AlphaZero，这次仅仅通过提供规则，而不学习任何人类棋谱，训练出来的AI能力就能轻松战胜其前辈AlphaGo。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">放在这里也是一样，我们的传统物理引擎，其实也是我们把人类研究到的物理底层规则变成了引擎，<b>但</b><b>这个世界可能还有众多的运行细节，我们根本没有关注到，所以为何不让AI自己去发现呢？</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">我刚才提到的这些，都是谷歌Genie 3发布的一些亮点。同学可能还会疑惑，这个世界模型与我们现在在用的AI工具到底有什么差别？</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">你可以理解现在有三个流派：</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第一个是大语言模型</b>，像我们现在常用的很多AI工具都是大语言模型，比如ChatGPT、豆包、DeepSeek等。这些工具类似于一台超级牛的自动补全写作机器，能够预测下一个词到底说什么，更能让用户满意，可以和我们协作、聊天和写报告。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第二个是多模态模型</b>，这个其实是很多AI大语言模型的发展方向。比如给豆包一张图让其修改成新图，或给一段视频让AI理解后在去生成新的文字或新的视频，还可以通过摄像头让AI理解实时画面。这些都让输入和输出，不再仅仅局限在文字上。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第三个就是世界模型</b>，有点像一个随时可进入的沙盒模拟器游戏，能够生成并持续模拟一个世界，用户或机器人可以走进去探索，走来走去、打怪或搬东西。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这三个流派各有特色，就像江湖中的各大门派，虽都是绝顶高手，只是大家的特点与方向各不相同。当然，这三个流派现在的实现难度也是不一样的，世界模型是一个更具挑战的领域，所以目前从画质到物理精度，都处在比较稚嫩的阶段，相当于游戏世界里还没有达到俄罗斯方块的水平，那就更别提《黑神话悟空》了。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">同学可能乍一听，觉得这怎么有点像游戏？如果有兴趣，你可以去了解一款名为《微软模拟飞行》的游戏。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">2011年，微软投入了3年时间和1.3亿美元，组建了专业航拍团队，使用特制飞机对北美和欧洲的17个国家进行全面航拍扫描，实现了30厘米的精度影像。这些高精度影像后来被整合进《微软模拟飞行2020》里。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这意味着，你虽然在游戏中开飞机，但是如果你放大画面，地面上一辆汽车的型号，或者一座建筑的窗户样子，都是可以清晰辨别的。更不用说飞机和机场的逼真程度，单单说最不起眼的树，游戏里内置了2万亿棵独立建模的树木，因为要让每棵树的种类、高度和枝叶分布符合当地的气候特征和光照条件。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">我提这些是想让大家意识到，<b>游戏有时不仅仅是用来消磨时间的</b>。比如，这个游戏常用于训练初级飞行员。因为传统的飞行员训练需要真实飞机，无法完整模拟真实场景，而在这款游戏中，可以模拟超过90%的真实飞行场景。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">说回世界模型，这个技术流派到底可以干什么？</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第一，我必须要说的是，可能每个人都能成为一个游戏设计师。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">随时想玩，随时写一个指令，我们可以在游戏里打怪升级，甚至探索100年后的赛博城市等设定。如果《微软模拟飞行》这种级别的游戏也能生成，那可能每个人探索游戏世界的方式会发生巨大的变化。比如，穿越小说可能很快就会变成一个依靠AI提示词的穿越游戏，是回到明朝当王爷，还是秦朝当小吏，全凭用户自己的选择。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第二，机器人的训练场。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这个应用场景已经不是未来，<b>某种意义上说，谷歌的这个世界模型正是为此而生</b>。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">现在机器人在实验室的研究发展迅速，但真要回到生活场景，每个家庭、每个工厂车间、每个仓库都是不一样的，因此需要让机器人在虚拟环境里不间断地尝试。比如让机器人可以在虚拟仓库里，每天24小时、每一秒都不停歇地搬运东西，在里面无限次的试错，那么等到进入真实世界，它们其实已经经历过无数次模拟、踩坑和纠错了。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第三，自动驾驶的压力测试。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">很早以前，有个做自动驾驶训练的工程师，回国和我吃饭。吃完饭，在路边等车的时候，他看着在北京国贸桥下穿梭的那些骑电动车的人们，眼神里放出贪婪的光，他说：“天啊，这是多好的训练数据啊。”</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">但有了世界模型，那么就可以在虚拟环境里，加入各种极端场景，比如暴风雨夜，路灯坏了，红绿灯也坏了，路上窜出五辆电动车、十辆摩托车、三头牛，再加上七个碰瓷老大爷，用这样的极端路况场景，测试自动驾驶车辆的反应能力。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第四，工厂或城市的数字孪生。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这个容易理解，就是建厂房之前，先在虚拟世界里面建好，从各种线路到各种具体型号的机器，都布好并且按照真实工厂的数据进行模拟生产，最后虚拟产量和运行流程达到了某个条件，才在真实世界里正式开工建造，这样的速度就非常快了。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">我之前问富士康的首席数字官史喆博士，他说他们富士康现在建任何的工厂，都是在数字世界里跑两三个月再正式动工，因为虚拟世界都磨合完了，所以从动工到真实生产，速度非常快。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>第五，对教育场景的大大拓展。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">我说的这个教育不仅仅是学校教育。例如，医学生可以在世界模型里实时生成病人的虚拟情况，模拟真实手术室，并在操作出现失误后立即进行复盘和研究。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">学机械或者学历史等等，这种模拟真实环境也可以带来不一样的体验。在过去，这些模拟环境每一个都需要专业的团队做长时间研发。但是如果AI世界模型进一步发展，一个历史老师就可以直接让AI模拟秦朝当时的条件，让学生体验从在家干农活到被抓走修长城的全过程。那学生们对历史上的每个小故事的理解，就会完全不一样了。当然，他们也能从那个环境里明白一个浅显的道理，那就是“人是哭不倒长城的”。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这里，我只是列出了几个显而易见的应用场景，再往下细分的话，还能衍生出各种使用的场景来。如果你对这一领域感兴趣，可以持续关注其进展。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">要知道我在广播站里经常提到的斯坦福大学的李飞飞教授，在去年进入了为期两年的休假状态，创办了一家公司，名为World Labs。从这个公司名就能看出她就是要打造世界模型，她在不少发言或者访谈中，都强调了一句话<b>“语言只是人类沟通工具，真正的智能要理解空间、物理和因果。”</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">好，今天广播就到这里。如果你觉得有帮助，欢迎分享转发给你的朋友。明天咱们接着聊AI。</p><div data-module-type="custom" class="split" style="z-index: 40; position: relative;"></div><figure data-module-type="custom" style="z-index: 40; position: relative;"><img src="https://piccdn2.umiwi.com/uploader/image/ddarticle/2025080720/1883954335290322780/080720.png" class="big-image"> <!----></figure> <!----><div style="user-select: none; z-index: 30; transition: top 0.1s, left 0.1s; display: none; position: absolute;"><div style="border-radius: 50%; margin: auto; background-color: rgb(255, 107, 0);"></div><div style="margin: auto; background-color: rgb(255, 107, 0);"></div><div style="border-radius: 50%; margin: auto;"></div></div><div style="user-select: none; z-index: 30; transition: top 0.1s, left 0.1s; display: none; position: absolute;"><div style="border-radius: 50%; margin: auto;"></div><div style="margin: auto; background-color: rgb(255, 107, 0);"></div><div style="border-radius: 50%; margin: auto; background-color: rgb(255, 107, 0);"></div></div><svg style="z-index: 20; pointer-events: none; width: 100%; height: 100%; position: absolute; top: 0px; left: 0px; overflow: visible;"></svg><svg style="z-index: 10; pointer-events: none; width: 100%; height: 100%; position: absolute; top: 0px; left: 0px; overflow: visible;"><line x1="561.3125" y1="501.5" x2="232.6875" y2="501.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.515625" y1="501.5" x2="550.3125" y2="501.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.5" y1="553.5" x2="-5.5" y2="553.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="187.5" y1="605.5" x2="-5.5" y2="605.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line></svg><div style="position: absolute; width: max-content; text-align: center; line-height: 0; z-index: 50; transform: translate3d(-50%, -100%, 0px); transition: transform 0.2s, opacity 0.2s; visibility: hidden; opacity: 0;"><div class="em-menu" style="font-size: 12px; background-color: rgb(38, 38, 38); border-radius: 6px; color: rgb(221, 221, 221); border: 0px; display: inline-block; padding: 14px 5px; margin: auto; height: 66px; line-height: 66px; cursor: pointer;"><span class="em-menu-item" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-note" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>写笔记</span><span class="em-menu-item em-menu-item-select" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-note-line" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>划线</span><span class="em-menu-item em-menu-item-highlight" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-delete-line" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>删除划线</span><span class="em-menu-item em-menu-item-copy" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-copy" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>复制</span></div><div class="em-menu-triangle" style="margin: -1px auto 10px; border-top: 8px solid rgb(38, 38, 38); border-right: 8px solid transparent; border-left: 8px solid transparent; width: 0px; height: 0px;"></div></div></div></div></div>

<div class="article-time-info iget-common-c3 iget-common-f4"><!----> <div class="article-publish-time">
                  首次发布: 2025年8月8日 
                </div></div>
</body>
</html>