<!doctype html>
<html lang="zh-Hant">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>646｜一张图能装下多少文字？DeepSeek-OCR探索AI的“遗忘机制”</title>
</head>
<body>
<div class="article-cover-wrap"><img src="https://piccdn3.umiwi.com/img/202510/22/202510221039255653953643.jpeg"> <!----> <!----></div>

<div class="article-title iget-common-c1" style="-webkit-box-orient: vertical;">
    646｜一张图能装下多少文字？DeepSeek-OCR探索AI的“遗忘机制”
  </div>

<div class="article-info"><div class="author"><img src="https://piccdn3.umiwi.com/img/202405/25/202405251635229233825579.jpeg"> <span class="course-title iget-common-c3 iget-common-f4">
        快刀广播站
      </span></div> <span class="article-publish-time iget-common-c3 iget-common-f4">
      2025年10月23日 
    </span></div>

<div class="article-body"><div class="iget_rich-text-panel--container iget_rich-text-panel__large"><div class="editor-show" style="user-select: none; position: relative;"><div class="dd-audio" data-module-type="custom" style="z-index: 40; position: relative;"><div class="dd-audio-player iget-common-b7"><div class="dd-audio-info"><button class="dd-audio-icon iget-common-b10"><span class="iconfont iget-common-f4 iget-icon-play"></span></button> <div class="dd-audio-block iget-common-c2 iget-common-f5"><span class="audio-title" style="-webkit-box-orient: vertical;">一张图能装下多少文字？DeepSeek-OCR探索AI的“遗忘机制”.mp3</span> <span class="audio-duration iget-common-c3 iget-common-f6">
          12分47秒
        </span></div></div></div> <div class="audio-tips iget-common-f4 iget-common-c3">
    转述师：AI
  </div></div><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">你好，我是快刀青衣。欢迎收听快刀广播站，每天带你看AI。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这两天，AI领域最火的当属DeepSeek的新模型。不过这次发布的只是个30亿参数的小模型，名叫<b>DeepSeek-OCR</b>。说实话，我刚看到这个名字时第一反应是非常失望——OCR这类技术不是早就被做透了吗？难道DeepSeek还想在这个领域，和扫描全能王这种文档扫描细分领域的霸主一较高低吗？</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">但一个最朴素的好奇心驱使我去研究了一下：DeepSeek员工的薪资可不低，这么顶尖的AI人才，不可能只满足于花精力做一件意义不大的事。这就像一位顶尖作家，如果在文章里写了一句看似多余的话，我的第一反应肯定是“大师这么写一定有隐含的深意，没看出来只能说明我水平不够”。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">看过论文之后我发现，人家压根不是为了做OCR而做OCR，而是在研究一个更根本的问题：<b>AI处理信息时，用“看图”的方式比“读文字”能省多少力气？</b>今天这期广播站，我会尽量用通俗的语言，和你分享一下我的理解和感受。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">先给不太了解AI的同学，稍微解释下“token”是怎么回事。你可以把token理解成AI处理信息的“计价单位”，就像去菜市场买菜，有的按斤卖、有的按个卖一样。一个中文字大概对应1-2个token，一个英文单词则可能是1-3个token。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">关键是，AI每处理一个token都要花钱，而且token越多，不仅花的钱越多，处理速度也会越慢。所以对AI来说，<b>怎么用更少的token传递更多信息，就成了一个很关键的问题。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">你看，咱们平时上网总说“一图胜千言，无图无真相”，但这话到底只是文学修辞，还是真能拿出具体数据来证明？DeepSeek这次的小模型，就是要把这个问题掰开、揉碎、研究清楚。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">他们通过实验发现：一篇包含约1000个“文字token”的文档，如果换成“看图”的方式处理，只用100个“视觉token”，就能以97%的精度还原出来。这是什么概念？<b>相当于把信息直接压缩了10倍，而且几乎没丢东西。</b>更厉害的是，即便把压缩比提到20倍，信息准确率还能保持在60%左右。这就有点像压缩饼干，占的体积小，但该有的“营养”还能保留不少，能管饱。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">不过这里要先明确一点：DeepSeek-OCR并不是真的像咱们用的扫描仪那样，先把文字转换成图片再处理，而是<b>借用了“视觉理解”的逻辑来处理信息</b>。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这就像人脑处理信息有不同的“通道”——有的信息适合用“听”来接收，比如一段音乐；有的信息适合用“看”来接收，比如一张图表。AI其实也类似，DeepSeek这次就发现，<b>面对文档这种文字密集的信息，用视觉模型的方式去编码、压缩，效率要比传统的纯文本处理方式要高效得多。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这里面的关键，是他们专门设计的一个叫<b>DeepEncoder</b>的编码器。至于“编码器”具体是什么，这里咱们不用深究，我给你打个比方就懂了：就像你整理衣柜，首先会把所有衣服都摆出来看一遍，然后把不常穿的衣服压缩装进真空袋，最后只把常穿的挂在外面，方便拿取。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">DeepEncoder也是这个思路：首先用“窗口注意力”，一块一块地看清楚内容；然后通过压缩模块，把信息直接压缩16倍；最后用“全局注意力”，提取出真正有用的核心知识。这样一套流程下来，既能把内容看明白，又不用消耗太多算力。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">单说压缩效果，咱们直接拿数据说话。现在市面上有个叫MinerU2.0的OCR模型，它是2025年8月上海人工智能实验室发布的开源工具，处理一页文档平均要用到6000多个token。而DeepSeek-OCR呢？只用不到800个token，就能超过它的效果。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这是什么概念？相当于别人得开着大卡车才能运完的货，DeepSeek开个小面包车就把活儿干完了，而且干得还更好。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">说到这儿，论文里还有个特别有意思的发现，这也是我认为整个研究最独特的洞察。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">DeepSeek的研究人员注意到，当他们把信息压缩比提高到20倍的时候，长文本在低分辨率下会变得模糊，识别精度会下降。但他们没把这个现象当成一个bug，反而从中看到了一个更深层的逻辑——<b>这不就是人类记忆的遗忘机制吗？</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">你想想，人的记忆是怎么工作的？刚发生的事，你记得一清二楚，连细节都丝毫不差；一小时前的事，还能记个大概；一天前的事，可能就只剩下关键情节了；再往远了说，一周前、一个月前、一年前的事，你的记忆会越来越模糊，最后可能只记得个大概印象。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这种记忆随时间衰退的过程，是不是和图像分辨率降低的样子特别像？<b>越久远的记忆，就像越模糊的图片，细节逐渐丢失，但核心信息还在。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">DeepSeek在论文里画了一张图，把这个类比展现得特别清楚，我把这张图贴到文稿里了。虽然论文里有很多张图表，但是这张图给我带来了极大的想法上的冲击。他们很巧妙地把时间维度和空间维度做了对比：</p><figure data-module-type="custom" style="z-index: 40; position: relative;"><img src="https://piccdn2.umiwi.com/uploader/image/ddarticle/2025102218/1890998711942215692/102218.png" class="big-image"> <!----></figure> <!----><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">对记忆来说，刚发生的事情是“水晶般清晰”，1小时前“非常清晰”，1天前“清晰”，到1周前开始“模糊”，1个月前已经“非常模糊”，1年前就“几乎消失”了。而在视觉上，10厘米距离看东西最清楚，接着是50厘米、1米、3米、10米、20米，距离越远，画面越模糊。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这个类比就给了他们一个大胆的想法：<b>能不能用这种</b><b>“</b><b>视觉压缩</b><b>”</b><b>的方式来模拟AI的记忆机制？</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">具体来说，就是把AI之前的对话历史，用不同分辨率的“视觉token”来存储。最近的对话用高分辨率，保留所有细节；稍微久一点的对话，就降低分辨率，压缩一些信息；更早的对话继续降低分辨率，只保留核心内容。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这样一来，AI就能像人一样，对近期的事情记得清清楚楚，对远期的事情只留模糊印象。这样既节省了计算资源，又符合我们使用AI的实际场景——毕竟大多数时候，我们只需要AI记住刚聊过的内容。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">说实话，当我看到这个想法时，第一时间对“醍醐灌顶”、“茅塞顿开”这样的词有了画面感。我的启发不只是来自这个小模型的效果，更是由衷感叹：有些人的思维方式，可能真的和普通人不一样。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">因为<b>这不仅仅是一次简单的技术优化，而是在探索AI如何像人一样“主动遗忘”。</b>你仔细想想，人为什么会遗忘？不是大脑容量不够，而是<b>遗忘本身就是一种智能。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">如果你对十年前的每一个细节都记得清清楚楚，那你的大脑早就被无用信息塞满了。遗忘其实是一种筛选机制——让重要的信息留下来，不重要的自然淡化。毕竟现在的你，没必要还清楚记得高三课堂上做过的那一道道数学题。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这一点也让我想到了费曼学习法。同学们都知道，费曼是全球顶级的物理学家，但在我看来，他还有一个同样重要的身份——“伟大的教育家”。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">他在加州理工学院任教近40年，留下过一个著名的检验标准。当时有同事请他解释量子力学的某个复杂问题，他说：“我没法把它简化到大学一年级的水平，这意味着实际上我们并不理解它。”</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">费曼的核心观点很明确：如果你没办法用简单的话把一个概念讲清楚，那就说明你自己还没真正理解它。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">你看，<b>费曼学习法的本质，其实也是一种“压缩”——把复杂零散的知识压缩成核心要点</b>。而且你会发现一个规律：刚学完的时候，你能复述出所有细节；过几天，可能只记得关键概念；再过几个月，你可能只记得最本质的原理。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">德国心理学家艾宾浩斯早在1885年就发现了这个规律，他的遗忘曲线明确显示：学习后20分钟，人会遗忘41.8%的内容；1天后，遗忘比例会达到66.3%。但这根本不是坏事，因为最后留下来的那个“最本质的原理”，才是真正能帮到你的有用知识。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">DeepSeek这套“视觉压缩”的思路，某种程度上就是在给AI量身设计一套类似的信息处理机制。让AI也能像人一样，对信息进行分层记忆：重要的、近期的信息，用高保真的方式完整保存；次要的、久远的信息，就通过压缩留存，只留下核心要点。<b>这不仅仅是一次普通的技术优化，而是在探索AI如何像人一样“智能地遗忘”。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">说到这儿，你可能会发现，DeepSeek团队有个特别有意思的特点：<b>他们从来不按常规套路出牌。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">别人做OCR，想的是怎么把文字识别得更准确；而DeepSeek做OCR，却在研究“一张图能压缩多少文字”这个更根本的问题。你再看他们的MoE架构，别人都在堆砌专家数量，DeepSeek却创新出“共享专家+路由专家”的组合，用更少的激活参数达到更好的效果。这次的OCR模型也是一样，模型只有30亿参数，激活参数才5.7亿，但效果却能超过那些动辄上百亿参数的大模型。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">之前在AI行业的一次闭门讨论里，我听过一句话，觉得特别准确：“DeepSeek这个团队把所有精力都放在了一个很窄的点，把后续很多东西都放弃了。他们不是单纯在服务人，而是做智能本身。”这话听着有些抽象，但你看他们的OCR模型就能明白了。他们的目标从来不是做一个好用的OCR工具，而是探索AI处理信息的本质。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这个模型发布后，业内的反响也挺有意思。最让我意外的是咱们广播站里经常提到的，前OpenAI联合创始人Andrej Karpathy的评价，他在X上说了一句特别颠覆的话：“也许更合理的是，LLM的所有输入都应该是图像。即使碰巧有纯文本输入，你更应该先渲染它，然后再输入。”</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">当然，这个模型不只是理论研究，实用价值也很明显。单张A100显卡一天能处理20万页文档，这意味着可以大规模地把PDF文档转成结构化的文本数据，用来训练大语言模型。而且因为它用的token少，处理速度快，成本也就低。同样的计算资源，DeepSeek能处理的数据量是别人的好几倍。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">说实话，这个研究让我看到了一个更大的想象空间。如果“视觉压缩”这个思路真的能用在长上下文处理上，那未来的AI可能会变成什么样？也许你跟AI聊了一整天，它不需要把所有对话都原封不动地保存，而是像人一样，把早上的对话压缩成低分辨率的“印象”，把中午的对话保留关键信息，只有最近的对话才保持高清晰度。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;"><b>这样一来，AI就能真正实现</b><b>“</b><b>无限对话</b><b>”</b><b>，不是靠无限的算力，而是靠智能的遗忘。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">回到开头那个问题：一张图能装下多少文字？DeepSeek用这个30亿参数的小模型告诉我们，答案不是一个固定的数字，而是取决于你愿意接受多少模糊度。10倍压缩几乎无损，20倍压缩还能保留核心信息。这个发现，可能比一个好用的OCR工具更有价值。因为它揭示了一个更本质的问题：<b>AI如何像人一样，在有限的资源里，智能地选择记住什么、遗忘什么。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">这才是DeepSeek真正在做的事情：<b>不是追着热点跑，而是在探索AI的本质。</b></p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">最后，我也给自己主理的AI学习圈打个广告，又到一年的双十一大促，也是入手AI学习圈年卡最划算的时候。<b>原价399元，现价349元，直降50元。</b>如果你之前就订阅了学习圈，觉得这个圈子对你有帮助，现在续费也是最划算的，扫描文稿末尾的二维码即可入手，谢谢你继续支持我们。</p><p data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line; z-index: 40; position: relative;">好，今天广播就到这里。如果你觉得有帮助，欢迎分享转发给你的朋友。明天咱们接着聊AI。</p><div data-module-type="custom" class="original-block-quote" style="z-index: 40; position: relative;"><blockquote data-module-type="internal" data-text-node="1" style="text-align: left; white-space: pre-line;"><b>相关链接：</b><br><b>1.论文</b>：https://arxiv.org/pdf/2510.18234<br><b>2.项目</b>：https://github.com/deepseek-ai/DeepSeek-OCR</blockquote></div> <!----> <!----><figure data-module-type="custom" style="z-index: 40; position: relative;"><img src="https://piccdn2.umiwi.com/uploader/image/ddarticle/2025102218/1890998915952562708/102218.jpeg" class="big-image"> <!----></figure> <!----><div style="user-select: none; z-index: 30; transition: top 0.1s, left 0.1s; display: none; position: absolute;"><div style="border-radius: 50%; margin: auto; background-color: rgb(255, 107, 0);"></div><div style="margin: auto; background-color: rgb(255, 107, 0);"></div><div style="border-radius: 50%; margin: auto;"></div></div><div style="user-select: none; z-index: 30; transition: top 0.1s, left 0.1s; display: none; position: absolute;"><div style="border-radius: 50%; margin: auto;"></div><div style="margin: auto; background-color: rgb(255, 107, 0);"></div><div style="border-radius: 50%; margin: auto; background-color: rgb(255, 107, 0);"></div></div><svg style="z-index: 20; pointer-events: none; width: 100%; height: 100%; position: absolute; top: 0px; left: 0px; overflow: visible;"></svg><svg style="z-index: 10; pointer-events: none; width: 100%; height: 100%; position: absolute; top: 0px; left: 0px; overflow: visible;"><line x1="825.5" y1="2463.5" x2="-5.5" y2="2463.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.5" y1="2515.5" x2="-5.5" y2="2515.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.5" y1="2567.5" x2="-5.5" y2="2567.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="291.5" y1="2619.5" x2="-5.5" y2="2619.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.5" y1="1861.5" x2="287.734375" y2="1861.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="483.28125" y1="1913.5" x2="-5.5" y2="1913.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.515625" y1="1913.5" x2="472.28125" y2="1913.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="83.5" y1="1965.5" x2="-5.5" y2="1965.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.5" y1="1539.5" x2="-5.5" y2="1539.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.5" y1="1591.5" x2="-5.5" y2="1591.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="250.640625" y1="1643.5" x2="-5.5" y2="1643.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.5" y1="1643.5" x2="239.640625" y2="1643.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="58" y1="1695.5" x2="-5.5" y2="1695.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.515625" y1="1695.5" x2="47" y2="1695.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="85.46875" y1="1747.5" x2="-5.5" y2="1747.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="245.171875" y1="823.5" x2="234.15625" y2="823.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="825.515625" y1="823.5" x2="234.171875" y2="823.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line><line x1="111.375" y1="875.5" x2="-5.5" y2="875.5" style="stroke: rgb(231, 130, 95); stroke-width: 1;"></line></svg><div style="position: absolute; width: max-content; text-align: center; line-height: 0; z-index: 50; transform: translate3d(-50%, -100%, 0px); transition: transform 0.2s, opacity 0.2s; visibility: hidden; opacity: 0;"><div class="em-menu" style="font-size: 12px; background-color: rgb(38, 38, 38); border-radius: 6px; color: rgb(221, 221, 221); border: 0px; display: inline-block; padding: 14px 5px; margin: auto; height: 66px; line-height: 66px; cursor: pointer;"><span class="em-menu-item" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-note" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>写笔记</span><span class="em-menu-item em-menu-item-select" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-note-line" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>划线</span><span class="em-menu-item em-menu-item-highlight" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-delete-line" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>删除划线</span><span class="em-menu-item em-menu-item-copy" style="line-height: 1.24rem; padding: 0px 15px; color: rgb(221, 221, 221); display: inline-block;"><span class="em-menu-item-icon iconfont iget-icon-copy" style="display: block; color: rgb(204, 204, 204); font-size: 20px; margin-bottom: 2px;"></span>复制</span></div><div class="em-menu-triangle" style="margin: -1px auto 10px; border-top: 8px solid rgb(38, 38, 38); border-right: 8px solid transparent; border-left: 8px solid transparent; width: 0px; height: 0px;"></div></div></div></div></div>

<div class="article-time-info iget-common-c3 iget-common-f4"><!----> <div class="article-publish-time">
                  首次发布: 2025年10月23日 
                </div></div>
</body>
</html>